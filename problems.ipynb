{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76337daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/11 09:04:15 WARN Utils: Your hostname, codespaces-7972f7, resolves to a loopback address: 127.0.0.1; using 10.0.2.146 instead (on interface eth0)\n",
      "25/08/11 09:04:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 09:04:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HogwartsAnalysis\").getOrCreate()\n",
    "\n",
    "data1 = [\n",
    "    (\"Gryffindor\", 1, 80, \"Harry Potter\"),\n",
    "    (\"Slytherin\", 1, 60, \"Draco Malfoy\"),\n",
    "    (\"Ravenclaw\", 1, 45, \"Luna Lovegood\"),\n",
    "    (\"Hufflepuff\", 1, 30, \"Cedric Diggory\"),\n",
    "    (\"Gryffindor\", 2, 90, \"Hermione Granger\"),\n",
    "    (\"Slytherin\", 2, 70, \"Pansy Parkinson\"),\n",
    "    (\"Ravenclaw\", 2, 55, \"Cho Chang\"),\n",
    "    (\"Hufflepuff\", 2, 65, \"Hannah Abbott\"),\n",
    "    (\"Gryffindor\", 3, 20, \"Ron Weasley\"),\n",
    "    (\"Slytherin\", 3, 85, \"Blaise Zabini\")\n",
    "]\n",
    "columns1 = [\"house\", \"year\", \"points\", \"student\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf5ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------------+\n",
      "|     house|year|total_points|\n",
      "+----------+----+------------+\n",
      "|Gryffindor|   1|          80|\n",
      "| Slytherin|   1|          60|\n",
      "|Gryffindor|   2|          90|\n",
      "| Slytherin|   2|          70|\n",
      "|Hufflepuff|   2|          65|\n",
      "| Ravenclaw|   2|          55|\n",
      "| Slytherin|   3|          85|\n",
      "+----------+----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result1 = (\n",
    "    df1.select(\"house\", \"year\", \"points\")\n",
    "       .filter(col(\"points\") > 50)\n",
    "       .groupBy(\"house\", \"year\")\n",
    "       .agg(_sum(\"points\").alias(\"total_points\"))\n",
    "       .orderBy(\"year\", col(\"total_points\").desc())\n",
    ")\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1a5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "data2 = [\n",
    "    (\"Aragorn\", \"Human\", 10, 2, \"Helms Deep\"),\n",
    "    (\"Legolas\", \"Elf\", 15, 0, \"Helms Deep\"),\n",
    "    (\"Gimli\", \"Dwarf\", 8, 3, \"Helms Deep\"),\n",
    "    (\"Frodo\", \"Hobbit\", 2, 1, \"Moria\"),\n",
    "    (\"Sam\", \"Hobbit\", 4, 2, \"Moria\"),\n",
    "    (\"Gandalf\", \"Wizard\", 12, 1, \"Moria\"),\n",
    "    (\"Boromir\", \"Human\", 7, 4, \"Amon Hen\"),\n",
    "    (\"Legolas\", \"Elf\", 20, 0, \"Amon Hen\"),\n",
    "    (\"Aragorn\", \"Human\", 9, 2, \"Amon Hen\")\n",
    "]\n",
    "columns2 = [\"name\", \"race\", \"enemies_defeated\", \"injuries\", \"battle\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954d4a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  race|avg_enemies_defeated|\n",
      "+------+--------------------+\n",
      "|   Elf|                17.5|\n",
      "|Wizard|                12.0|\n",
      "| Human|   8.666666666666666|\n",
      "| Dwarf|                 8.0|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result2 = (\n",
    "    df2.select(\"name\", \"race\", \"enemies_defeated\")\n",
    "       .filter(col(\"enemies_defeated\") > 5)\n",
    "       .groupBy(\"race\")\n",
    "       .agg(avg(\"enemies_defeated\").alias(\"avg_enemies_defeated\"))\n",
    "       .orderBy(col(\"avg_enemies_defeated\").desc())\n",
    ")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c3f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum as _sum\n",
    "data3 = [\n",
    "    (\"SUB-01\", \"Pacific Strike\", 5, \"Success\"),\n",
    "    (\"SUB-02\", \"Atlantic Surge\", 2, \"Failure\"),\n",
    "    (\"SUB-01\", \"Arctic Blitz\", 4, \"Success\"),\n",
    "    (\"SUB-03\", \"Indian Ocean\", 6, \"Success\"),\n",
    "    (\"SUB-02\", \"Pacific Strike\", 3, \"Success\"),\n",
    "    (\"SUB-01\", \"Coral Sea\", 7, \"Success\"),\n",
    "    (\"SUB-03\", \"Arctic Blitz\", 1, \"Failure\"),\n",
    "    (\"SUB-02\", \"Bering Strait\", 5, \"Success\")\n",
    "]\n",
    "columns3 = [\"submarine_id\", \"mission_name\", \"warheads_launched\", \"status\"]\n",
    "df3 = spark.createDataFrame(data3, columns3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c21827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+-----------------------+\n",
      "|submarine_id|total_missions|total_warheads_launched|\n",
      "+------------+--------------+-----------------------+\n",
      "|      SUB-01|             3|                     16|\n",
      "|      SUB-03|             1|                      6|\n",
      "|      SUB-02|             1|                      5|\n",
      "+------------+--------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result3 = (\n",
    "    df3.select(\"submarine_id\", \"mission_name\", \"warheads_launched\")\n",
    "       .filter(col(\"warheads_launched\") > 3)\n",
    "       .groupBy(\"submarine_id\")\n",
    "       .agg(\n",
    "            count(\"mission_name\").alias(\"total_missions\"),\n",
    "            _sum(\"warheads_launched\").alias(\"total_warheads_launched\")\n",
    "       )\n",
    "       .orderBy(col(\"total_warheads_launched\").desc())\n",
    ")\n",
    "result3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
