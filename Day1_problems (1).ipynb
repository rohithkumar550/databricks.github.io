{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a997e0ef-a9dc-4e1b-8b44-7a9187842cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerData\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81b9441-4334-4f17-b7c7-9ce75d9c2b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS retail.customer_data\")\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"signup_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "customer_data = [\n",
    "    (1, \"Alice Johnson\", \"West\", \"Active\", \"2023-01-15\"),\n",
    "    (2, \"Bob Smith\", \"East\", \"Inactive\", \"2023-02-20\"),\n",
    "    (3, \"Charlie Lee\", \"West\", \"Active\", \"2023-03-10\")\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "customers_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retail.customer_data.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88918b60-b243-4693-a3d6-cfa04e04bc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+\n|customer_id|         name|region|\n+-----------+-------------+------+\n|          1|Alice Johnson|  West|\n|          3|  Charlie Lee|  West|\n+-----------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT customer_id, name, region\n",
    "    FROM retail.customer_data.customers\n",
    "    WHERE status = 'Active' AND region = 'West'\n",
    "\"\"\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba346f54-81ce-4fd8-8d5a-2e1dedea9400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "spark = SparkSession.builder.appName(\"InventoryUpdates\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad45072-1d5c-4d35-a56c-a28b3019af3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ecommerce.inventory\")\n",
    "\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"stock\", IntegerType(), True),\n",
    "    StructField(\"warehouse\", StringType(), True)\n",
    "])\n",
    "\n",
    "initial_data = [\n",
    "    (101, \"Laptop\", 50, \"A\"),\n",
    "    (102, \"Smartphone\", 100, \"B\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=inventory_schema)\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.inventory.products\")\n",
    "\n",
    "update_data = [\n",
    "    (101, \"Laptop\", 45, \"A\"),\n",
    "    (103, \"Tablet\", 30, \"A\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d875b943-3899-493a-9069-d0354c5ce9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+---------+\n|product_id|      name|stock|warehouse|\n+----------+----------+-----+---------+\n|       101|    Laptop|   45|        A|\n|       102|Smartphone|  100|        B|\n|       103|    Tablet|   30|        A|\n+----------+----------+-----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "update_df = spark.createDataFrame(update_data, schema=inventory_schema)\n",
    "\n",
    "target = DeltaTable.forName(spark, \"ecommerce.inventory.products\")\n",
    "(\n",
    "    target.alias(\"target\")\n",
    "    .merge(\n",
    "        update_df.alias(\"source\"),\n",
    "        \"target.product_id = source.product_id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(set={\"stock\": \"source.stock\"})\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"stock\": \"source.stock\",\n",
    "        \"warehouse\": \"source.warehouse\"\n",
    "    })\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "spark.sql(\"SELECT * FROM ecommerce.inventory.products ORDER BY product_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3e520b-2324-419a-ae6a-5e78c7bf6fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from delta.tables import DeltaTable\n",
    "spark = SparkSession.builder.appName(\"SalesAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3f2459-6cda-4973-8df0-beca4c247827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS finance.sales\")\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), False),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"quarter\", StringType(), True)\n",
    "])\n",
    "\n",
    "initial_data = [\n",
    "    (1, \"Stocks\", 10000, \"Q1-2023\"),\n",
    "    (2, \"Bonds\", 15000, \"Q1-2023\")\n",
    "]\n",
    "initial_df = spark.createDataFrame(initial_data, schema=sales_schema)\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"finance.sales.quarterly_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3812cc-111c-46a5-a534-7d84ef7ba687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Version - Total Amount Before Update:\n+------------+\n|total_amount|\n+------------+\n|       25000|\n+------------+\n\nCurrent Version - Total Amount After Update:\n+------------+\n|total_amount|\n+------------+\n|       27000|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "update_data = [\n",
    "    (1, \"Stocks\", 12000, \"Q1-2023\")\n",
    "]\n",
    "update_df = spark.createDataFrame(update_data, schema=sales_schema)\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, \"finance.sales.quarterly_sales\")\n",
    "delta_table.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.sale_id = source.sale_id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"amount\": \"source.amount\"}\n",
    ").execute()\n",
    "\n",
    "history_df = delta_table.history()\n",
    "previous_version = history_df.select(\"version\").collect()[1][0]\n",
    "\n",
    "previous_total_df = spark.sql(\n",
    "    f\"SELECT SUM(amount) AS total_amount FROM finance.sales.quarterly_sales VERSION AS OF {previous_version}\"\n",
    ")\n",
    "print(\"Previous Version - Total Amount Before Update:\")\n",
    "previous_total_df.show()\n",
    "\n",
    "current_total_df = spark.sql(\n",
    "    \"SELECT SUM(amount) AS total_amount FROM finance.sales.quarterly_sales\"\n",
    ")\n",
    "print(\"Current Version - Total Amount After Update:\")\n",
    "current_total_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day1_problems",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}